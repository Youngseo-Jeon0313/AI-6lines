# AI-육목

# 구현 계획

2022.01.04
alpha zero 논문 참고. 기계가 선생처럼 자신이 플레이하는 게임의 룰도 따로 학습하고 자신의 policy network도 꾸준히 update해 나가는 기계학습을 시킬 예정이다.
코드를 분류해보자면 일단,

//mcts.py
mcts tree 구조를 만든다.
tree를 방문할 때 expand, select, backpropagation, evaluation을 기준으로 코드를 나누어 작성한다.
너무 어려운 ..코드는 아직 초보단계인 나에게 너무 어려울 듯 하여 가장 이해가 잘되는 
http://joshvarty.github.io/AlphaZero/ 이 2목(??) 코드를 완전히 이해한 후 작성하려 한다.
아무래도 alphazero의 3가지 핵심 부분은
1. value network
2. policy network
3. monte carlo tree search 
이며, 이 때 3에다가 1,2를 어떻게 조화롭게 넣느냐에 초점을 맞추어 코드를 짤 생각이다.

일단 2목, Muzero와 관련한 논문/코드를 읽어보았고 조금 참고해서 코드를 짜볼 생각이다.
GPU에 관련한 내용은 아직 좀 잘 몰라서(이게 컴퓨터 메모리와 관련한건가?? 좀 잘 모르겠다) 공부를 좀 하다가 구현이 GPU와 관련되는 게 너무 힘들 경우 육목판을
조금 줄이든 그런 식으로 진행할 생각이다.

육목판의 경우 참고해서 구현은 해놓았으나 첫 수는 1번 2번째부터는 2번씩 놓을 수 있도록 구현하도록 바꾸어야 하고,
game.py에 환경설정한 {0,0,0,0,}으로의 tuple값이나 또는 numpy 2차원 배열을 이용해서 html 육목판에 구현하는 것도 조금 배워야 한다 일단은 ㅠㅠ

2022.01.05

MCTS.py 2목 버전을 함께 이해해보았다. 아무래도 강화학습/alpha zero 파트는 혼자 읽으면 아오씨 싶은 게 같이 하면 그거아냐 저거아냐 하면서 되게 이해가 잘 되는 것 같아서 아주 뿌듯-★
암튼 MCTS 과정 4단계는 잘 이해한 듯 하다.
다만 조금 삔또(?)가 나는 건 배열 안에 append 또는 expand를 어떻게 시키는지, 그리고 node를 바꿔주는 과정에서의 상대성(즉 child 하나로 가면 거기서 또 node를 다시 생각하는 건지)이 조금 헷갈린다. 근데 그건 s -> s' -> s''으로 생각하니 좀 이해가 된듯.
내일은 이제 Trainer로 들어가서 어떻게 Train 시키는지에 관한 내용을 익힐 예정이다. 

2022.01.14
강화학습 CNN part는 아직 보고서 하지 않는 이상 내가 파파박 못 짜겠다ㅠ
물론 보고 하는 것도 아주 훌륭하지만,, 내 말은 아예 배끼는 거ㅠㅠ
프로젝트라고 하면 생각이 녹아들어가야 하는데 그게 충족되지 않아서 고민이다.
일단은 내가 짜고 있는 코드에 대해서 완벽히 이해한 후 그걸 알기 쉽게 설명한다면 그것만으로도 성공..!
혹은 MCTS로 짤 수 있는 것 중 조금 쉬운 걸 ㅠㅠㅠ 도전해 봐야 할 것 같다.
그리고 코드는 보고 또 보면 또 감이 느는 게 있으니까 그냥 드립다(?) 볼 생각이다.

+js에 파이썬 알고리즘을 넣어야 하는데 그것조차도 그리 쉽지 않다ㅠㅠㅠ javascript에서 python코드를 호출하는 방법에 대해 또 공부를 해야 할 것 같다.




